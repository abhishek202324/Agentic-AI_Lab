{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9663ad9-217c-46d3-befc-8ccabeba890b",
   "metadata": {
    "id": "a9663ad9-217c-46d3-befc-8ccabeba890b"
   },
   "source": [
    "# 5 Levels Of Text Splitting\n",
    "\n",
    "Ever try to put a long piece of text into ChatGPT but it tells you it’s too long? Or you're trying to give your application better long term memory, but it’s still just not quite working.\n",
    "\n",
    "One of the most effective strategies to improve performance of your language model applications is to split your large data into smaller pieces. This is call splitting or chunking (we'll use these terms interchangeably). In the world of multi-modal, splitting also applies to images.\n",
    "\n",
    "We are going to cover a lot, but if you make it to the end, I guarantee you’ll have a solid grasp on chunking theory, strategies, and resources to learn more.\n",
    "\n",
    "**Levels Of Text Splitting**\n",
    "* **Level 1: [Character Splitting](#CharacterSplitting)** - Simple static character chunks of data\n",
    "* **Level 2: [Recursive Character Text Splitting](#RecursiveCharacterSplitting)** - Recursive chunking based on a list of separators\n",
    "* **Level 3: [Document Specific Splitting](#DocumentSpecific)** - Various chunking methods for different document types (PDF, Python, Markdown)\n",
    "* **Level 4: [Semantic Splitting](#SemanticChunking)** - Embedding walk based chunking\n",
    "* **Level 5: [Agentic Splitting](#AgenticChunking)** - Experimental method of splitting text with an agent-like system.\n",
    "\n",
    "\n",
    "This tutorial will use code from LangChain (`pip install langchain`)\n",
    "\n",
    "**Evaluations**\n",
    "\n",
    "It's important to test your chunking strategies in retrieval evals. It doesn't matter how you chunk if the performance of your application isn't great.\n",
    "\n",
    "Eval Frameworks:\n",
    "\n",
    "* [LangChain Evals](https://python.langchain.com/docs/guides/evaluation/)\n",
    "* [Llama Index Evals](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html)\n",
    "* [RAGAS Evals](https://github.com/explodinggradients/ragas)\n",
    "\n",
    "\n",
    "## Level 1: Character Splitting <a id=\"CharacterSplitting\"></a>\n",
    "Character splitting is the most basic form of splitting up your text. It is the process of simply dividing your text into N-character sized chunks regardless of their content or form.\n",
    "\n",
    "This method isn't recommended for any applications - but it's a great starting point for us to understand the basics.\n",
    "\n",
    "* **Pros:** Easy & Simple\n",
    "* **Cons:** Very rigid and doesn't take into account the structure of your text\n",
    "\n",
    "Concepts to know:\n",
    "* **Chunk Size** - The number of characters you would like in your chunks. 50, 100, 100,000, etc.\n",
    "* **Chunk Overlap** - The amount you would like your sequential chunks to overlap. This is to try to avoid cutting a single piece of context into multiple pieces. This will create duplicate data across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96299fc-30f5-4edf-ac23-23a29f9c7282",
   "metadata": {
    "id": "c96299fc-30f5-4edf-ac23-23a29f9c7282"
   },
   "outputs": [],
   "source": [
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1yTEJINz96MA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1yTEJINz96MA",
    "outputId": "65cd3e87-95f7-43d8-f853-a31c04811611"
   },
   "outputs": [],
   "source": [
    "chunks=[]\n",
    "\n",
    "chunks_size=20\n",
    "\n",
    "for i in range(0, len(text),chunks_size):\n",
    "    chunk = text[i:i + chunks_size]\n",
    "    chunks.append(chunk)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XHMIgsod_2Gr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHMIgsod_2Gr",
    "outputId": "8cdb3350-90a5-44cc-9720-3106a2be9963"
   },
   "outputs": [],
   "source": [
    "len('This is the text I w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1cf67e-98d7-48bd-9867-f72be72e3f4a",
   "metadata": {
    "id": "4e1cf67e-98d7-48bd-9867-f72be72e3f4a"
   },
   "source": [
    "Then let's split this text manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11fb88f-17ed-44c2-b4de-a8a527fe63c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f11fb88f-17ed-44c2-b4de-a8a527fe63c7",
    "outputId": "2abbfaf3-49af-46a4-b91e-4eb43617ad08"
   },
   "outputs": [],
   "source": [
    "# Create a list that will hold your chunks\n",
    "chunks = []\n",
    "\n",
    "chunk_size = 35 # Characters\n",
    "\n",
    "# Run through the a range with the length of your text and iterate every chunk_size you want\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunk = text[i:i + chunk_size]\n",
    "    chunks.append(chunk)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140085b7-c6af-4003-923c-73feb1825965",
   "metadata": {
    "id": "140085b7-c6af-4003-923c-73feb1825965"
   },
   "source": [
    "When working with text in the language model world, we don't deal with raw strings. It is more common to work with documents. Documents are objects that hold the text you're concerned with, but also additional metadata which makes filtering and manipulation easier later.\n",
    "\n",
    "\n",
    "Let's load up LangChains `CharacterSplitter` to do this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EYNm39joKAMi",
   "metadata": {
    "id": "EYNm39joKAMi"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_y0vwgmWJJLG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_y0vwgmWJJLG",
    "outputId": "f47cf7ba-dde8-4995-955f-96d98ff86ab2"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbeb8d-c5a0-4047-8250-967313c20935",
   "metadata": {
    "id": "3dcbeb8d-c5a0-4047-8250-967313c20935"
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=0, separator='', strip_whitespace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BJ8oi8YO4x9P",
   "metadata": {
    "id": "BJ8oi8YO4x9P"
   },
   "outputs": [],
   "source": [
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dA0GJ--Gu9Et",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dA0GJ--Gu9Et",
    "outputId": "10443bdb-a8a4-4de0-b3e6-70c22ed4022d"
   },
   "outputs": [],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae25bbe-d7d1-44da-820b-3cd34a1cfc67",
   "metadata": {
    "id": "5ae25bbe-d7d1-44da-820b-3cd34a1cfc67"
   },
   "source": [
    "Then we can actually split our text via `create_documents`. Note: `create_documents` expects a list of texts, so if you just have a string (like we do) you'll need to wrap it in `[]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331025f4-6ef4-4459-bcb6-df7824e78ce4",
   "metadata": {
    "id": "331025f4-6ef4-4459-bcb6-df7824e78ce4"
   },
   "source": [
    "Notice how this time we have the same chunks, but they are in documents. Also notice how the trailing whitespace on the end of the 2nd chunk is missing. This is because LangChain removes it, see [this line](https://github.com/langchain-ai/langchain/blob/f36ef0739dbb548cabdb4453e6819fc3d826414f/libs/langchain/langchain/text_splitter.py#L167) for where they do it. You can avoid this with `strip_whitespace=False`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed0f193-4098-4fb1-a42f-7f96cd182188",
   "metadata": {
    "id": "1ed0f193-4098-4fb1-a42f-7f96cd182188"
   },
   "source": [
    "**Chunk Overlap & Separators**\n",
    "\n",
    "**Chunk overlap** will blend together our chunks so that the tail of Chunk #1 will be the same thing and the head of Chunk #2 and so on and so forth.\n",
    "\n",
    "This time I'll load up my overlap with a value of 4, this means 4 characters of overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66f496-7b0d-4b2a-a43d-e8f06d58c934",
   "metadata": {
    "id": "fc66f496-7b0d-4b2a-a43d-e8f06d58c934"
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=4, separator='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d7e36-b592-430e-9069-cc025c78d7ef",
   "metadata": {
    "id": "fd5d7e36-b592-430e-9069-cc025c78d7ef"
   },
   "outputs": [],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4aaa8-b90b-499e-b2d5-bc623b5bb751",
   "metadata": {
    "id": "dcd4aaa8-b90b-499e-b2d5-bc623b5bb751"
   },
   "source": [
    "Notice how we have the same chunks, but now there is overlap between 1 & 2 and 2 & 3. The 'o ch' on the tail of Chunk #1 matches the 'o ch' of the head of Chunk #2.\n",
    "\n",
    "I wanted a better way to visualize this, so I made [ChunkViz.com](www.chunkviz.com) to help show it. Here's what the same text looks like.\n",
    "\n",
    "**Separators** are character(s) sequences you would like to split on. Say you wanted to chunk your data at `ch`, you can specify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ce9aa-17c3-4205-b433-2eae612c2225",
   "metadata": {
    "id": "814ce9aa-17c3-4205-b433-2eae612c2225"
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=0, separator='ch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb759b1f-dab0-4f5e-a0c0-220374313da6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb759b1f-dab0-4f5e-a0c0-220374313da6",
    "outputId": "79bf11f7-a775-4bef-e120-eaa3907ed438"
   },
   "outputs": [],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539aa1a2-67b8-4585-b0d3-306703ea856b",
   "metadata": {
    "id": "539aa1a2-67b8-4585-b0d3-306703ea856b"
   },
   "source": [
    "## Level 2: Recursive Character Text Splitting\n",
    "<a id=\"RecursiveCharacterSplitting\"></a>\n",
    "\n",
    "The problem with Level #1 is that we don't take into account the structure of our document at all. We simply split by a fix number of characters.\n",
    "\n",
    "The Recursive Character Text Splitter helps with this. With it, we'll specify a series of separatators which will be used to split our docs.\n",
    "\n",
    "You can see the default separators for LangChain [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L842). Let's take a look at them one by one.\n",
    "\n",
    "* \"\\n\\n\" - Double new line, or most commonly paragraph breaks\n",
    "* \"\\n\" - New lines\n",
    "* \" \" - Spaces\n",
    "* \"\" - Characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f42bea-3d06-404d-9f8c-f15f7ff7591b",
   "metadata": {
    "id": "49f42bea-3d06-404d-9f8c-f15f7ff7591b"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772695d-0c5e-4e19-bb69-14e9bd7a15a7",
   "metadata": {
    "id": "0772695d-0c5e-4e19-bb69-14e9bd7a15a7"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n",
    "\n",
    "Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\n",
    "\n",
    "It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb158c-6bbe-4f49-95df-a8b43965a566",
   "metadata": {
    "id": "9fbb158c-6bbe-4f49-95df-a8b43965a566"
   },
   "source": [
    "Now let's make our text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ec54c4-bda6-4254-97dd-983775b1d729",
   "metadata": {
    "id": "03ec54c4-bda6-4254-97dd-983775b1d729"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 65, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4I5nPbY7tiQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4I5nPbY7tiQ",
    "outputId": "51722601-e30c-464c-ae48-2fde29a186bc"
   },
   "outputs": [],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c7676-1e67-4084-94d3-59689eb399c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "887c7676-1e67-4084-94d3-59689eb399c6",
    "outputId": "ebcfb82f-49b6-4db8-89be-d4a203dda557"
   },
   "outputs": [],
   "source": [
    "docs= text_splitter.create_documents([text])\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "  doc.metadata={\"source_file\":\"file.txt\",\"chunk_no\":i}\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa00043-1655-4113-bb28-f3a998d5713a",
   "metadata": {
    "id": "3fa00043-1655-4113-bb28-f3a998d5713a"
   },
   "source": [
    "Notice how now there are more chunks that end with a period \".\". This is because those likely are the end of a paragraph and the splitter first looks for double new lines (paragraph break).\n",
    "\n",
    "Once paragraphs are split, then it looks at the chunk size, if a chunk is too big, then it'll split by the next separator. If the chunk is still too big, then it'll move onto the next one and so forth.\n",
    "\n",
    "For text of this size, let's split on something bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da8734e-47da-4a08-8459-9bf8bfed7fe4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6da8734e-47da-4a08-8459-9bf8bfed7fe4",
    "outputId": "9dc7f425-d457-49a2-cea5-d92b0277b91a"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 450, chunk_overlap=0)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99768f-8732-44e4-b8d8-cc5ac1fe4661",
   "metadata": {
    "id": "1e99768f-8732-44e4-b8d8-cc5ac1fe4661"
   },
   "source": [
    "For this text, 450 splits the paragraphs perfectly. You can even switch the chunk size to 469 and get the same splits. This is because this splitter builds in a bit of cushion and wiggle room to allow your chunks to 'snap' to the nearest separator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f32a73-0c8a-498c-a3a1-3e7dba4658c9",
   "metadata": {
    "id": "c5f32a73-0c8a-498c-a3a1-3e7dba4658c9"
   },
   "source": [
    "## Level 3: Document Specific Splitting <a id=\"DocumentSpecific\"></a>\n",
    "\n",
    "Start to handle document types other than normal prose in a .txt.\n",
    "\n",
    "This level is all about making your chunking strategy fit your different data formats. Let's run through a bunch of examples of this in action\n",
    "\n",
    "The Markdown, Python, and JS splitters will basically be similar to Recursive Character, but with different separators.\n",
    "\n",
    "See all of LangChains document splitters [here](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter)\n",
    "### Markdown\n",
    "\n",
    "You can see the separators [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L1175).\n",
    "\n",
    "Separators:\n",
    "* `\\n#{1,6}` - Split by new lines followed by a header (H1 through H6)\n",
    "* ```` ```\\n ```` - Code blocks\n",
    "* `\\n\\\\*\\\\*\\\\*+\\n` - Horizontal Lines\n",
    "* `\\n---+\\n` - Horizontal Lines\n",
    "* `\\n___+\\n` - Horizontal Lines\n",
    "* `\\n\\n` Double new lines\n",
    "* `\\n` - New line\n",
    "* `\" \"` - Spaces\n",
    "* `\"\"` - Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298fe868-0872-4fa9-9146-fa33e9dd5706",
   "metadata": {
    "id": "298fe868-0872-4fa9-9146-fa33e9dd5706"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d390ed-d046-44f9-a492-9760141f7982",
   "metadata": {
    "id": "e1d390ed-d046-44f9-a492-9760141f7982"
   },
   "outputs": [],
   "source": [
    "splitter = MarkdownTextSplitter(chunk_size = 40, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba14168-451b-4e9c-b1d0-d1eac6996ad3",
   "metadata": {
    "id": "1ba14168-451b-4e9c-b1d0-d1eac6996ad3"
   },
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"\n",
    "# Fun in California\n",
    "\n",
    "## Driving\n",
    "\n",
    "Try driving on the 1 down to San Diego\n",
    "\n",
    "### Food\n",
    "\n",
    "Make sure to eat a burrito while you're there\n",
    "\n",
    "## Hiking\n",
    "\n",
    "Go to Yosemite\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dcf8de-551a-4477-8e68-57c4c50ddbc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15dcf8de-551a-4477-8e68-57c4c50ddbc4",
    "outputId": "b66479b6-db2f-4eab-fca3-be21fcf4dfb9"
   },
   "outputs": [],
   "source": [
    "splitter.create_documents([markdown_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56591620-ef0c-41c2-b539-35ad676ed20f",
   "metadata": {
    "id": "56591620-ef0c-41c2-b539-35ad676ed20f"
   },
   "source": [
    "Notice how the splits gravitate towards markdown sections. However, it's still not perfect. Check out how there is a chunk with just \"there\" in it. You'll run into this at low-sized chunks.\n",
    "\n",
    "### Python\n",
    "\n",
    "See the python splitters [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L1069)\n",
    "\n",
    "* `\\nclass` - Classes first\n",
    "* `\\ndef` - Functions next\n",
    "* `\\n\\tdef` - Indented functions\n",
    "* `\\n\\n` - Double New lines\n",
    "* `\\n` - New Lines\n",
    "* `\" \"` - Spaces\n",
    "* `\"\"` - Characters\n",
    "\n",
    "\n",
    "Let's load up our splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66edcde5-1e96-4b61-8636-8129d31d7850",
   "metadata": {
    "id": "66edcde5-1e96-4b61-8636-8129d31d7850"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import PythonCodeTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa8f93-6b07-484f-86ff-9836f5a5fae1",
   "metadata": {
    "id": "2afa8f93-6b07-484f-86ff-9836f5a5fae1"
   },
   "outputs": [],
   "source": [
    "python_text = \"\"\"\n",
    "class Person:\n",
    "  def __init__(self, name, age):\n",
    "    self.name = name\n",
    "    self.age = age\n",
    "\n",
    "p1 = Person(\"John\", 36)\n",
    "\n",
    "for i in range(10):\n",
    "    print (i)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8fcc85-714d-4b5c-a5ce-a3f30cfb447b",
   "metadata": {
    "id": "6e8fcc85-714d-4b5c-a5ce-a3f30cfb447b"
   },
   "outputs": [],
   "source": [
    "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b6dd89-6bb9-496a-a85d-3f1871ff9cd0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7b6dd89-6bb9-496a-a85d-3f1871ff9cd0",
    "outputId": "82f03f44-5076-45a3-a2cb-c640e8b0d233"
   },
   "outputs": [],
   "source": [
    "python_splitter.create_documents([python_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c004f19-9e67-451e-abdd-b103acce2996",
   "metadata": {
    "id": "6c004f19-9e67-451e-abdd-b103acce2996"
   },
   "source": [
    "### JS\n",
    "\n",
    "Very similar to python. See the separators [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L983).\n",
    "\n",
    "Separators:\n",
    "* `\\nfunction` - Indicates the beginning of a function declaration\n",
    "* `\\nconst` - Used for declaring constant variables\n",
    "* `\\nlet` - Used for declaring block-scoped variables\n",
    "* `\\nvar` - Used for declaring a variable\n",
    "* `\\nclass` - Indicates the start of a class definition\n",
    "* `\\nif` - Indicates the beginning of an if statement\n",
    "* `\\nfor` - Used for for-loops\n",
    "* `\\nwhile` - Used for while-loops\n",
    "* `\\nswitch` - Used for switch statements\n",
    "* `\\ncase` - Used within switch statements\n",
    "* `\\ndefault` - Also used within switch statements\n",
    "* `\\n\\n` - Indicates a larger separation in text or code\n",
    "* `\\n` - Separates lines of code or text\n",
    "* `\" \"` - Separates words or tokens in the code\n",
    "* `\"\"` - Makes every character a separate element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5225b66-4d79-455b-92a1-841fa23ccc4f",
   "metadata": {
    "id": "a5225b66-4d79-455b-92a1-841fa23ccc4f"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75090fa-4d22-4348-8452-eb50eafa784d",
   "metadata": {
    "id": "d75090fa-4d22-4348-8452-eb50eafa784d"
   },
   "outputs": [],
   "source": [
    "javascript_text = \"\"\"\n",
    "// Function is called, the return value will end up in x\n",
    "let x = myFunction(4, 3);\n",
    "\n",
    "function myFunction(a, b) {\n",
    "// Function returns the product of a and b\n",
    "  return a * b;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909fde28-43ba-4f07-b9ae-04c21db04055",
   "metadata": {
    "id": "909fde28-43ba-4f07-b9ae-04c21db04055"
   },
   "outputs": [],
   "source": [
    "js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.JS, chunk_size=65, chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70d936-bc31-4ecc-b190-6dd8fffdacb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b70d936-bc31-4ecc-b190-6dd8fffdacb9",
    "outputId": "fe03881b-2767-4bb3-f25c-b62661a1af5d"
   },
   "outputs": [],
   "source": [
    "js_splitter.create_documents([javascript_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NBQMpfvCaC6I",
   "metadata": {
    "id": "NBQMpfvCaC6I"
   },
   "source": [
    "## Level 4: Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MMokmN99Jb7E",
   "metadata": {
    "id": "MMokmN99Jb7E"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain_experimental langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kq1O6Mq1Mnlt",
   "metadata": {
    "id": "kq1O6Mq1Mnlt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from google.colab import userdata\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E1dHVYRUIuD6",
   "metadata": {
    "id": "E1dHVYRUIuD6"
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JOj7M1koIqg7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOj7M1koIqg7",
    "outputId": "fea4f995-6ed1-444b-cfeb-1eea2dc73c62"
   },
   "outputs": [],
   "source": [
    "# Same Tesla text but structured to show semantic grouping\n",
    "tesla_text = \"\"\"Tesla's Q3 Results\n",
    "Tesla reported record revenue of $25.2B in Q3 2024.\n",
    "The company exceeded analyst expectations by 15%.\n",
    "Revenue growth was driven by strong vehicle deliveries.\n",
    "\n",
    "Model Y Performance\n",
    "The Model Y became the best-selling vehicle globally, with 350,000 units sold.\n",
    "Customer satisfaction ratings reached an all-time high of 96%.\n",
    "Model Y now represents 60% of Tesla's total vehicle sales.\n",
    "\n",
    "Production Challenges\n",
    "Supply chain issues caused a 12% increase in production costs.\n",
    "Tesla is working to diversify its supplier base.\n",
    "New manufacturing techniques are being implemented to reduce costs.\"\"\"\n",
    "\n",
    "# Semantic Chunker - groups by meaning, not structure\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=70\n",
    ")\n",
    "\n",
    "chunks = semantic_splitter.split_text(tesla_text)\n",
    "\n",
    "print(\"SEMANTIC CHUNKING RESULTS:\")\n",
    "print(\"=\" * 50)\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}: ({len(chunk)} chars)\")\n",
    "    print(f'\"{chunk}\"')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MlVBulvjVKRd",
   "metadata": {
    "id": "MlVBulvjVKRd"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Tesla reported record revenue of $25.2B in Q3 2024.\",\n",
    "    \"The company exceeded analyst expectations by 15%.\",\n",
    "    \"Revenue growth was driven by strong vehicle deliveries.\",\n",
    "    \"The Model Y became the best-selling vehicle globally, with 350,000 units sold.\",\n",
    "    \"Customer satisfaction ratings reached an all-time high of 96%.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vZj9p943Vu0f",
   "metadata": {
    "id": "vZj9p943Vu0f"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "client = OpenAI()\n",
    "# Generate embeddings\n",
    "embeddings = []\n",
    "for s in sentences:\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=s\n",
    "    )\n",
    "    embeddings.append(response.data[0].embedding)\n",
    "\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ew0stlASVna6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ew0stlASVna6",
    "outputId": "f4a7e59b-6eeb-47b6-82f0-d2d3d44cb912"
   },
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print(f\"Similarity S1 vs S2: {similarity_matrix[0][1]:.3f}\")\n",
    "print(f\"Similarity S2 vs S3: {similarity_matrix[1][2]:.3f}\")\n",
    "print(f\"Similarity S3 vs S4: {similarity_matrix[2][3]:.3f}\")\n",
    "print(f\"Similarity S4 vs S5: {similarity_matrix[3][4]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sNtL-IoVaFvI",
   "metadata": {
    "id": "sNtL-IoVaFvI"
   },
   "source": [
    "## Level 5: Agentic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xYdeuJMsZmnd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYdeuJMsZmnd",
    "outputId": "b22b9d3d-b8dd-449b-dd03-89d6ad8eb693"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\", temperature=0)\n",
    "\n",
    "# Tesla text to chunk\n",
    "tesla_text = \"\"\"Tesla's Q3 Results\n",
    "Tesla reported record revenue of $25.2B in Q3 2024.\n",
    "The company exceeded analyst expectations by 15%.\n",
    "Revenue growth was driven by strong vehicle deliveries.\n",
    "\n",
    "Model Y Performance\n",
    "The Model Y became the best-selling vehicle globally, with 350,000 units sold.\n",
    "Customer satisfaction ratings reached an all-time high of 96%.\n",
    "Model Y now represents 60% of Tesla's total vehicle sales.\n",
    "\n",
    "Production Challenges\n",
    "Supply chain issues caused a 12% increase in production costs.\n",
    "Tesla is working to diversify its supplier base.\n",
    "New manufacturing techniques are being implemented to reduce costs.\"\"\"\n",
    "\n",
    "# Create the prompt\n",
    "prompt = f\"\"\"\n",
    "You are a text chunking expert. Split this text into logical chunks.\n",
    "\n",
    "Rules:\n",
    "- Each chunk should be around 200 characters or less\n",
    "- Split at natural topic boundaries\n",
    "- Keep related information together\n",
    "- Put \"<<<SPLIT>>>\" between chunks\n",
    "\n",
    "Text:\n",
    "{tesla_text}\n",
    "\n",
    "Return the text with <<<SPLIT>>> markers where you want to split:\n",
    "\"\"\"\n",
    "\n",
    "# Get AI response\n",
    "print(\"Asking AI to chunk the text...\")\n",
    "response = llm.invoke(prompt)\n",
    "marked_text = response.content\n",
    "\n",
    "# Split the text at the markers\n",
    "chunks = marked_text.split(\"<<<SPLIT>>>\")\n",
    "\n",
    "# Clean up the chunks (remove extra whitespace)\n",
    "clean_chunks = []\n",
    "for chunk in chunks:\n",
    "    cleaned = chunk.strip()\n",
    "    if cleaned:  # Only keep non-empty chunks\n",
    "        clean_chunks.append(cleaned)\n",
    "\n",
    "# Show results\n",
    "print(\"\\nAGENTIC CHUNKING RESULTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, chunk in enumerate(clean_chunks, 1):\n",
    "    print(f\"Chunk {i}: ({len(chunk)} chars)\")\n",
    "    print(f'\"{chunk}\"')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kFRrW6gpQTap",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kFRrW6gpQTap",
    "outputId": "2e6a3331-4ac1-4ebf-da17-5ae9faa5a4c2"
   },
   "outputs": [],
   "source": [
    "!pip install -Uq \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CAIjGGxwPQvA",
   "metadata": {
    "id": "CAIjGGxwPQvA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.staging.base import elements_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9t36Ju6OQ8FQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466,
     "referenced_widgets": [
      "73888281dcc94051b85f4d8a2e96f004",
      "db703672afd541aaa8d568393e0e0111",
      "d3510bf8544c46b293e0257cf3b5c1c1",
      "49fd852c70e1424a9058a3a1e293ed05",
      "4c698660c90e479489245f57227152c4",
      "3b9f70482d374a039fafb6e1fcb22e81",
      "ba9f89edf86d4708bc36727b352021b0",
      "4303efd9d61a4f5b90a8fe1f8f01f49e",
      "e3838a13e3ab45128879d384bcd2edf9",
      "273d8f656850432182202f80f01cfa6e",
      "fac90ca0b175464a8d5028e2b371a397",
      "04062aece47a4c60829887fd4fc63683",
      "04b127f5603749b783fdee1138e8e282",
      "63bb07c4bb38437c8aa42310fc963b78",
      "de6666d67d7b4a65afd3200185c88dd6",
      "fc530a9d87b443da8cd9b74a2c20710e",
      "053fc12262ca497aa9f5798e5f04df4e",
      "b2270ecae9ef405c8a6b65e15f84bc8c",
      "8aa2e6c4da6b4afab8383aa750eb47ce",
      "2caeadab2e3840f59b92a853af4c6665",
      "77c3171ad37d4d5382784adfd8de1171",
      "aa6f0781a6394fe4b70b6e1e6654d19f",
      "af4d08d114674adeab20547cc53d255c",
      "63dc3f1a8522489d9d43e6d23082fc53",
      "752e4167340a42c489c8c48bb08fd306",
      "d6a7933b0c97407d80cda54ec4e8e79e",
      "571d1dd9a19c4a1789c97fececb7d8f6",
      "9143d2be94aa47dbb0e025c386e456ad",
      "86544db69fcb4bccb1abe63a939bd409",
      "21975e446fdf4687b0b2a884b002dba8",
      "960d6438a8db408382dae257a101dc9f",
      "bf5b22e4afdc4a2bad66c2c2331d8b63",
      "4b41a4e97d6d472dafb5f7589335d8ba",
      "a78b11dfa1334b1fb1903d724ddbb79b",
      "d376dcf7c037441c802b7d414f5b15e5",
      "ee60eb57128749469effab72e1cd1a3f",
      "a172fccf86dd4a2289b4ce66319eaa05",
      "e87c2482ee0a4947835f65bd482aa446",
      "39bc921a4a5845d4bf6e24d34bbcb2d0",
      "3a71647405984c80a6f84f3f653ec67e",
      "0a25b9830df8475584349ab74acc1705",
      "0be75bbd9400461ebc797216874406bf",
      "6e025d807d2e43b99d1e4b5085f24fa7",
      "7333b079d6e94d998d1895824c0b69aa"
     ]
    },
    "id": "9t36Ju6OQ8FQ",
    "outputId": "32864d2f-f176-4d70-a0ac-89df7bd508f6"
   },
   "outputs": [],
   "source": [
    "filename = \"/content/attention-is-all-you-need.pdf\"\n",
    "\n",
    "# Install poppler-utils\n",
    "!apt-get install -y poppler-utils\n",
    "\n",
    "# Extracts the elements from the PDF\n",
    "elements = partition_pdf(\n",
    "    filename=filename,\n",
    "\n",
    "    # Unstructured Helpers\n",
    "    strategy=\"hi_res\",\n",
    "    infer_table_structure=True,\n",
    "    model_name=\"yolox\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7G51C5epRegn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7G51C5epRegn",
    "outputId": "5e37d311-972a-4b2b-f9e2-7ad2281b65c6"
   },
   "outputs": [],
   "source": [
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5dYcnSNSe4G",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t5dYcnSNSe4G",
    "outputId": "5022aca2-9604-40f2-aaa2-0c0b2ac0ed72"
   },
   "outputs": [],
   "source": [
    "from unstructured.documents.elements import Table\n",
    "\n",
    "tables = [el for el in elements if isinstance(el, Table)]\n",
    "\n",
    "for i, table in enumerate(tables):\n",
    "    print(f\"\\n--- Table {i+1} ---\")\n",
    "    print(table.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QWu3Pc5EVrHy",
   "metadata": {
    "id": "QWu3Pc5EVrHy"
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NqawQ2h8VvL7",
   "metadata": {
    "id": "NqawQ2h8VvL7"
   },
   "outputs": [],
   "source": [
    "filepath = \"/content/attention-is-all-you-need.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YXw2zcmSWJJH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXw2zcmSWJJH",
    "outputId": "4853f700-8625-49c0-a846-ca62bcff6ce6"
   },
   "outputs": [],
   "source": [
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=filepath,\n",
    "\n",
    "    # Using pdf format to find embedded image blocks\n",
    "    extract_images_in_pdf=True,\n",
    "\n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "\n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    # Hard max on chunks\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=\"/content/pdfImages/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E6uy-hKTWwlB",
   "metadata": {
    "id": "E6uy-hKTWwlB"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LIkxH0MCW1jz",
   "metadata": {
    "id": "LIkxH0MCW1jz"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-vision-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CLJtvURRW5Uh",
   "metadata": {
    "id": "CLJtvURRW5Uh"
   },
   "outputs": [],
   "source": [
    "# Function to convert image to base64\n",
    "def image_to_base64(image_path):\n",
    "    with Image.open(image_path) as image:\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=image.format)\n",
    "        img_str = base64.b64encode(buffered.getvalue())\n",
    "        return img_str.decode('utf-8')\n",
    "\n",
    "image_str = image_to_base64(\"/content/figures/figure-4-3.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GeARNTllYCVi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "GeARNTllYCVi",
    "outputId": "77cbcafd-2715-40ca-ff31-8eb38ec504e9"
   },
   "outputs": [],
   "source": [
    "image_str"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
