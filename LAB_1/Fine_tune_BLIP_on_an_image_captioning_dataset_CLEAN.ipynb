{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KNaYcxCqn8t"
   },
   "source": [
    "## Set-up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqrW65lfqhhJ",
    "outputId": "f595b591-16db-487b-ea6d-0ede33254774"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTI8wKxgql9i"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmUzlJNHq0CK"
   },
   "source": [
    "## Load the image captioning dataset\n",
    "\n",
    "Let's load the image captioning dataset, you just need few lines of code for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240,
     "referenced_widgets": [
      "27df20b3c0fe4560a5a3e01ad212b63e",
      "3d563cacb4934cbdabcb91c857756f74",
      "eaa028c91be94a848016763b45905880",
      "f3e5433efb794bd09e28f15b8ed25d59",
      "5fdced356eca48dcbfee3312355a7bcf",
      "984a316432e749b7951c90b8d313902e",
      "648bca35643f46b8bcce5ddbf4050e57",
      "ef00dff364de47a399317c2b2e736ba0",
      "f4c19970d8054d1999b5dbcc69641f29",
      "3b0ba520c2ed492fa7478788e18c9356",
      "e09ade14b28c447995bec8564352b1c6",
      "385c57c0bc08463c81ddaeadcdcf299b",
      "30ff60a3799f4289b9c7f35ebcfe8d3d",
      "412bd388693046e19ffeec3b569d0b49",
      "9fb726a0b0764edfa46bb142e99a8c57",
      "ad00206f21eb487a810e1c68e65ac7b8",
      "ea9a8f59a3b046059caa77df93767922",
      "b9487a79d66b4b2ab3a8634fb038fb12",
      "733e2d8b2bde49fcaf7c55cb7a1d3cd4",
      "95daf773675744748cbc9906192ce5e2",
      "e5938058dc0f47308eb5b98a12c1b21d",
      "f7b4790760c74c28b50d811c43d26fd0",
      "5dda39cff623442f900304257df7307e",
      "bac0d9002dc1402dbe6b9052dd096c92",
      "9f11d1ca9276440dbbb9d1e28397c38e",
      "819820aa640d48b2960b38ed192f114c",
      "bba49c0ad97647c597c451cc2a8c9883",
      "8d5bd667d2934114b3e289cd9ba28103",
      "b69b33b9d77f4a3a8d932f73c852fc46",
      "f3b4e92a72b542a99d2d8bf84c3ce174",
      "7b33b1b1d5434af5b1d2cdcd219273c8",
      "3e23c21498e743af9fd63e305c94f32e",
      "d498bd865b2142f3803424d2983fe18b"
     ]
    },
    "id": "QwkaKQKLs8vT",
    "outputId": "a29c46f9-14ec-4d3f-dcde-7570d6af5b2f"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ybelkada/football-dataset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y52jgY9-mhvJ"
   },
   "source": [
    "Let's retrieve the caption of the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "n4WLPzYlutg6",
    "outputId": "c9121db3-8c05-4fa9-ee1d-19a6039a763b"
   },
   "outputs": [],
   "source": [
    "dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukZ5x_Sumn1f"
   },
   "source": [
    "And the corresponding image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "lxoDUv5ymqBJ",
    "outputId": "819b775c-7c9f-47e6-f945-dd71d550ba55"
   },
   "outputs": [],
   "source": [
    "dataset[0][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSWkkqiKqrhv"
   },
   "source": [
    "## Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBjZLhLFmwwv"
   },
   "source": [
    "The lines below are entirely copied from the original notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93od71o_qq_V"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        encoding = self.processor(images=item[\"image\"], text=item[\"text\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "        # remove batch dimension\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VX6Nv8aEm0yk"
   },
   "source": [
    "## Load model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248,
     "referenced_widgets": [
      "76c170fa11e64eabad6a1038b72ba975",
      "d3863ad3d36d4190a0f7cc7dd6997104",
      "3340e8ca26a946a2a252e30117015343",
      "243861597ce6472482abbfd064a17338",
      "185032abaf0741328d3a50658b68f4c8",
      "b8d2f546947642b7806a4e1b26e72619",
      "b4f8178f4e8a4941883af69c4430dd16",
      "d2b0ea7f484f4a7bb2f973beb966effa",
      "74a97a9e23ef44719f4771d5b5bdff3a",
      "464496cd3169456b800327a685dddd01",
      "360ea77c51f841bd9157d3bca91de7b6"
     ]
    },
    "id": "hLhbdBLNxBuF",
    "outputId": "775bcaa3-300e-49e2-92c1-61447fc9664d"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXlkVag6nDx3"
   },
   "source": [
    "Now that we have loaded the processor, let's load the dataset and the dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxajSwc3w-LU"
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptioningDataset(dataset, processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_CyLSgBxyL2"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cVsJdd2nV1S"
   },
   "source": [
    "Let's train the model! Run the simply the cell below for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cCVhsmJxxjH",
    "outputId": "50deb56f-00e3-4943-9323-21d683861f82"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(50):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch.pop(\"input_ids\").to(device)\n",
    "    pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    labels=input_ids)\n",
    "\n",
    "    loss = outputs.loss\n",
    "\n",
    "    print(\"Loss:\", loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNZQXZrERyQN"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiWpn-q1oFu0"
   },
   "source": [
    "Let's check the results on our train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "uC-Fp480XAt7",
    "outputId": "1bdf1d67-f896-4fd3-dc5f-1310cbdb8260"
   },
   "outputs": [],
   "source": [
    "# load image\n",
    "example = dataset[0]\n",
    "image = example[\"image\"]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6P3-u0eRxmsa",
    "outputId": "9a02093a-bc74-4e0f-9f97-7bae144c9368"
   },
   "outputs": [],
   "source": [
    "# prepare image for the model\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
